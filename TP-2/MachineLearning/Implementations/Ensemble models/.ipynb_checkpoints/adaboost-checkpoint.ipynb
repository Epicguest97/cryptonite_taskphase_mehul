{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39550bfa-d936-461a-919e-d695e6b7a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8057\n",
      "Testing Accuracy: 0.7930\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        feature_values = X[:, self.feature_idx]\n",
    "        return np.where(feature_values < self.threshold if self.polarity == 1 \n",
    "                       else feature_values > self.threshold, -1, 1)\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.stumps = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "            \n",
    "        # Convert y to {-1, 1} once\n",
    "        y_mean = np.mean(y)\n",
    "        y_ = np.where(y > y_mean, 1, -1)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            stump = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "            \n",
    "            # Randomly select features to consider (33% of features)\n",
    "            n_features_to_try = max(1, n_features // 3)\n",
    "            feature_indices = np.random.choice(n_features, n_features_to_try, replace=False)\n",
    "            \n",
    "            for feature_idx in feature_indices:\n",
    "                feature_values = X[:, feature_idx]\n",
    "                \n",
    "                # Use percentile-based thresholds instead of all unique values\n",
    "                thresholds = np.percentile(feature_values, [25, 50, 75])\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    for polarity in [-1, 1]:\n",
    "                        predictions = np.where(feature_values < threshold if polarity == 1 \n",
    "                                            else feature_values > threshold, -1, 1)\n",
    "                        \n",
    "                        error = np.sum(sample_weights * (y_ != predictions))\n",
    "                        \n",
    "                        if error < min_error:\n",
    "                            min_error = error\n",
    "                            stump.polarity = polarity\n",
    "                            stump.threshold = threshold\n",
    "                            stump.feature_idx = feature_idx\n",
    "            \n",
    "            # Calculate stump weight (alpha)\n",
    "            EPS = 1e-10\n",
    "            stump.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "            \n",
    "            # Update sample weights\n",
    "            predictions = stump.predict(X)\n",
    "            sample_weights *= np.exp(-stump.alpha * y_ * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)  # Normalize\n",
    "            \n",
    "            self.stumps.append(stump)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for stump in self.stumps:\n",
    "            predictions += stump.alpha * stump.predict(X)\n",
    "        return np.sign(predictions)\n",
    "\n",
    "# Load and prepare a subset of the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "# Use only 5000 samples for faster execution\n",
    "n_samples = 5000\n",
    "indices = np.random.choice(len(data.data), n_samples, replace=False)\n",
    "X, y = data.data[indices], data.target[indices]\n",
    "\n",
    "# Split and scale the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate\n",
    "adaboost = AdaBoost(n_estimators=20)  # Reduced number of estimators\n",
    "adaboost.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = adaboost.predict(X_train_scaled)\n",
    "y_pred_test = adaboost.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(np.where(y_train > np.mean(y_train), 1, -1) == y_pred_train)\n",
    "test_accuracy = np.mean(np.where(y_test > np.mean(y_test), 1, -1) == y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd23c6-10b3-4cbd-88f6-b276bb14b4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
