{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd6db585-0f45-4d79-9d23-84464eaf2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MEDIAN_VALUE  \n",
      "0    -122.23         4.526  \n",
      "1    -122.22         3.585  \n",
      "2    -122.24         3.521  \n",
      "3    -122.25         3.413  \n",
      "4    -122.25         3.422  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# Convert to a DataFrame\n",
    "data = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "data['MEDIAN_VALUE'] = california.target\n",
    "\n",
    "# Display the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f72b3d75-4158-47cb-9279-36214e123476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute the initial prediction (base prediction, for regression)\n",
    "def initialize_predictions(y):\n",
    "    return np.mean(y)\n",
    "\n",
    "# Function to compute residuals (errors) - for regression\n",
    "def compute_residuals(y, predictions):\n",
    "    return y - predictions\n",
    "\n",
    "# Function to compute the gradient for regression\n",
    "def compute_gradient(y, predictions):\n",
    "    return compute_residuals(y, predictions)\n",
    "\n",
    "# Simple decision tree (regression) as weak learner\n",
    "def fit_tree(X, residuals, max_depth=3):\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    tree.fit(X, residuals)\n",
    "    return tree\n",
    "\n",
    "# Update the model with the new learner's predictions\n",
    "def update_predictions(predictions, tree, X, learning_rate):\n",
    "    tree_preds = tree.predict(X)\n",
    "    return predictions + learning_rate * tree_preds\n",
    "\n",
    "# XGBoost-like model (simplified version)\n",
    "class SimplifiedXGBoost:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.initial_prediction = initialize_predictions(y)\n",
    "        predictions = np.full_like(y, self.initial_prediction, dtype=np.float32)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = compute_gradient(y, predictions)\n",
    "            tree = fit_tree(X, residuals, self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "            predictions = update_predictions(predictions, tree, X, self.learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.full_like(X[:, 0], self.initial_prediction, dtype=np.float32)\n",
    "        for tree in self.trees:\n",
    "            predictions = update_predictions(predictions, tree, X, self.learning_rate)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ef72cc7-630f-48f3-b963-d0a0ced81c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model using the Normal Equation.\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        y: Target vector (n_samples,)\n",
    "        \"\"\"\n",
    "        # Add bias term (intercept) to X\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add a column of ones to X\n",
    "        \n",
    "        # Compute weights using the Normal Equation\n",
    "        self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values using the fitted model.\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Add bias term (intercept) to X\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Compute predictions\n",
    "        return X @ self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97659821-06f1-4f2c-a426-5b638831670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 220.3890651617063\n"
     ]
    }
   ],
   "source": [
    "# Generating a simple regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "\n",
    "# Train the simplified XGBoost model\n",
    "model = SimplifiedXGBoost(learning_rate=0.1, n_estimators=50, max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y, predictions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "957c5ba5-1c5f-496b-a797-276c713efc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9902311212685999\n"
     ]
    }
   ],
   "source": [
    "def r2_score(y_true, y_pred):\n",
    "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    residual_variance = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (residual_variance / total_variance)\n",
    "# Compute R-squared\n",
    "r2 = r2_score(y, predictions)\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de824c0-3338-4af5-be9a-c0f6a182382f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
