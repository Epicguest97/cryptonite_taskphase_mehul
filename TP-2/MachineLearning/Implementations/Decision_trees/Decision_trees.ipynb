{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50337620-dd90-403f-bc93-f46b069b933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8b1e9682-77ce-41be-b987-574dc4e60bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n",
      "(120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Iris.csv')\n",
    "\n",
    "# Drop unnecessary columns (if any)\n",
    "if 'Id' in data.columns:\n",
    "    data.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Convert 'species' to numeric labels\n",
    "Species_map = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "data['Species'] = data['Species'].map(Species_map)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('Species', axis=1).values\n",
    "y = data['Species'].values\n",
    "\n",
    "# Set the test size (20% for testing, 80% for training)\n",
    "test_size = 0.2\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(41)  # for reproducibility\n",
    "indices = np.random.permutation(len(X))\n",
    "\n",
    "# Calculate the split index\n",
    "test_size_index = int(len(X) * test_size)\n",
    "\n",
    "# Split the indices\n",
    "train_indices = indices[test_size_index:]\n",
    "test_indices = indices[:test_size_index]\n",
    "\n",
    "# Split the features and target arrays\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "47800288-b7b8-4f34-84f6-80dc1b19e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad62c8a8-b6c7-43b5-9803-9ad32f5d7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def split_data(self, dataset, feature, threshold):\n",
    "        # Create empty arrays to store the left and right datasets\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # Loop over each row in the dataset and split based on the given feature and threshold\n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "\n",
    "        # Convert the left and right datasets to numpy arrays and return\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        return left_dataset, right_dataset\n",
    "\n",
    "    def entropy(self, y):\n",
    "        entropy = 0\n",
    "\n",
    "        # Find the unique label values in y and loop over each value\n",
    "        labels = np.unique(y)\n",
    "        for label in labels:\n",
    "            # Find the examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of the current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # Calculate the entropy using the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        # Return the final entropy value\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, parent, left, right):\n",
    "        # set initial information gain to 0\n",
    "        information_gain = 0\n",
    "        # compute entropy for parent\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        # calculate weight for left and right nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right= len(right) / len(parent)\n",
    "        # compute entropy for left and right nodes\n",
    "        entropy_left, entropy_right = self.entropy(left), self.entropy(right)\n",
    "        # calculate weighted entropy \n",
    "        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "        # calculate information gain \n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        # dictionary to store the best split values\n",
    "        best_split = {'gain':- 1, 'feature': None, 'threshold': None}\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            #get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            #get unique values of that feature\n",
    "            thresholds = np.unique(feature_values)\n",
    "            # loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # get left and right datasets\n",
    "                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)\n",
    "                # check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    # compute information gain based on the y values\n",
    "                    information_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "\n",
    "    \n",
    "    def calculate_leaf_value(self, y):\n",
    "        y = list(y)\n",
    "        #get the highest present class in the array\n",
    "        most_occuring_value = max(y, key=y.count)\n",
    "        return most_occuring_value\n",
    "    \n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # keeps spliting until stopping conditions are met\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best_split = self.best_split(dataset, n_samples, n_features)\n",
    "            # Check if gain isn't zero\n",
    "            if best_split[\"gain\"]:\n",
    "                # continue splitting the left and the right child. Increment current depth\n",
    "                left_node = self.build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "                right_node = self.build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "        # compute leaf node value\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        # return leaf node value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        dataset = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self.make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        # if the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None: \n",
    "            return node.value\n",
    "        else:\n",
    "            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a300b68f-8c03-4818-8603-7b4c911d3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    total_samples = len(y_true)\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    return (correct_predictions / total_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0699754c-a29e-4809-bd8e-df3f4f8e535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = y_true.flatten()\n",
    "    # Get the number of classes\n",
    "    n_classes = len(np.unique(y_true))\n",
    "\n",
    "    # Initialize an array to store the sensitivity and specificity for each class\n",
    "    sen = []\n",
    "    spec = []\n",
    "    # Loop over each class\n",
    "    for i in range(n_classes):\n",
    "        # Create a mask for the true and predicted values for class i\n",
    "        mask_true = y_true == i\n",
    "        mask_pred = y_pred == i\n",
    "\n",
    "        # Calculate the true positive, true negative, false positive, and false negative values\n",
    "        TP = np.sum(mask_true & mask_pred)\n",
    "        TN = np.sum((mask_true != True) & (mask_pred != True))\n",
    "        FP = np.sum((mask_true != True) & mask_pred)\n",
    "        FN = np.sum(mask_true & (mask_pred != True))\n",
    "\n",
    "        # Calculate the sensitivity (true positive rate) and specificity (true negative rate)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "\n",
    "        # Store the sensitivity and specificity for class i\n",
    "        sen.append(sensitivity)\n",
    "        spec.append(specificity)\n",
    "    # Calculate the balanced accuracy as the average of the sensitivity and specificity for each class\n",
    "    average_sen =  np.mean(sen)\n",
    "    average_spec =  np.mean(spec)\n",
    "    balanced_acc = (average_sen + average_spec) / n_classes\n",
    "\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "116dedb8-6386-46ad-bed7-a1d7b8715c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Accuracy: 0.9\n",
      "Model's Balanced Accuracy: 0.6196969696969697\n"
     ]
    }
   ],
   "source": [
    "#create model instance\n",
    "model = DecisionTree(2, 2)\n",
    "\n",
    "# Fit the decision tree model to the training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluating metrics\n",
    "print(f\"Model's Accuracy: {accuracy(y_test, predictions)}\")\n",
    "print(f\"Model's Balanced Accuracy: {balanced_accuracy(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95475a85-43d9-4611-83cf-f145a770261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1(conf_matrix):\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1_score = np.zeros(num_classes)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        precision[i] = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if precision[i] + recall[i] > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a27469e3-48bf-471d-a0cf-26e00a46095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate Precision, Recall, F1-Score for each class\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m precision, recall, f1_score \u001b[38;5;241m=\u001b[39m precision_recall_f1(conf_matrix)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision per class:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall per class:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recall)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set (using your model's predict method)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f'Accuracy: {acc:.2f}')\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score for each class\n",
    "precision, recall, f1_score = precision_recall_f1(conf_matrix)\n",
    "\n",
    "print(\"Precision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1-Score per class:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbff44-2007-4467-8e8d-76d13f599656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
