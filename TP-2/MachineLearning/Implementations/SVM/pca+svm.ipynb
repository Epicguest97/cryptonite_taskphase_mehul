{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519df64b-78eb-4cd6-b16b-427b56876c31",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM): A Detailed Overview\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning algorithms used for classification and regression tasks. They are particularly effective in high-dimensional spaces and for problems with clear margins of separation. This repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f7d0b-af42-4392-bf86-079ad42c1ab3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Support Vector Machines aim to find the optimal hyperplane that separates data into different classes. For non-linearly separable data, SVM employs kernel functions to map the data into a higher-dimensional space where a linear separator can be found.\n",
    "\n",
    "\n",
    "## SVM for Binary Classification\n",
    "\n",
    "### Hyperplane and Decision Boundary\n",
    "\n",
    "The hyperplane is the decision boundary that maximizes the margin between two classes. In a binary classification problem, the goal of SVM is to find the hyperplane defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w}$: Weight vector.\n",
    "- $\\mathbf{x}$: Feature vector.\n",
    "- $b$: Bias term.\n",
    "\n",
    "The classes are separated as:\n",
    "- $\\mathbf{w}^T \\mathbf{x} + b > 0$ for class $+1$\n",
    "- $\\mathbf{w}^T \\mathbf{x} + b < 0$ for class $-1$\n",
    "\n",
    "### Mathematics of SVM\n",
    "\n",
    "The margin is the distance between the hyperplane and the nearest data points from each class (support vectors). The optimization problem can be formulated as:\n",
    "\n",
    "#### Objective Function:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "#### Subject to:\n",
    "\n",
    "$$\n",
    "y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\|\\mathbf{w}\\|$ is the norm of the weight vector (controls margin width).\n",
    "- $y_i \\in \\{-1, +1\\}$ are the class labels.\n",
    "- $\\mathbf{x}_i$ are the feature vectors.\n",
    "\n",
    "\n",
    "## Kernel Trick\n",
    "\n",
    "For non-linearly separable data, the kernel trick maps the input features into a higher-dimensional space, allowing a linear hyperplane to separate the data. The kernel function computes the dot product in the transformed feature space without explicitly computing the transformation.\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "1. **Linear Kernel:**\n",
    "   $$\n",
    "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n",
    "   $$\n",
    "\n",
    "2. **Polynomial Kernel:**\n",
    "   $$\n",
    "   K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d\n",
    "   $$\n",
    "\n",
    "3. **Gaussian (RBF) Kernel:**\n",
    "   $$\n",
    "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "\n",
    "4. **Sigmoid Kernel:**\n",
    "   $$\n",
    "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\alpha \\mathbf{x}_i^T \\mathbf{x}_j + c)\n",
    "   $$\n",
    "\n",
    "\n",
    "## Soft Margin and Regularization\n",
    "\n",
    "Real-world data is often noisy and non-linearly separable. To handle such cases, SVM introduces a soft margin, allowing some misclassifications. The optimization problem becomes:\n",
    "\n",
    "#### Objective Function:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "#### Subject to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\\\\n",
    "& \\xi_i \\geq 0 \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\xi_i$: Slack variable representing the degree of misclassification.\n",
    "- $C$: Regularization parameter that controls the trade-off between margin width and classification error.\n",
    "\n",
    "\n",
    "## Mathematics of SVM Optimization\n",
    "\n",
    "To solve the optimization problem, SVM uses the Lagrange multipliers. The dual form of the optimization problem is:\n",
    "\n",
    "#### Dual Objective Function:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "#### Subject to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n",
    "& 0 \\leq \\alpha_i \\leq C \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_i$: Lagrange multipliers.\n",
    "- $K(\\mathbf{x}_i, \\mathbf{x}_j)$: Kernel function.\n",
    "\n",
    "The decision function is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ce0b9a-333f-4eba-87a3-4a94968449e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Titanic dataset\n",
    "file_path = 'titanic.csv'  # Adjust this if needed\n",
    "titanic_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cd9b4-9998-46f6-a305-8152bc8248a1",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing\n",
    "1. Drop irrelevant features\n",
    "2. Handle missing values\n",
    "3. Encode categorical variables\n",
    "4. Separate features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5341a847-05a2-4783-8db1-bed0fb2b6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_cleaned = titanic_data.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "titanic_data_cleaned['Age'].fillna(titanic_data_cleaned['Age'].mean(), inplace=True)  # Fill Age with mean\n",
    "titanic_data_cleaned['Embarked'].fillna('missing', inplace=True)  # Fill Embarked with placeholder\n",
    "\n",
    "titanic_data_encoded = pd.get_dummies(titanic_data_cleaned, drop_first=True)\n",
    "\n",
    "X = titanic_data_encoded.drop(columns=['Survived'])\n",
    "y = titanic_data_encoded['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eed2fe-1b7c-449e-a947-56c27431c79c",
   "metadata": {},
   "source": [
    "#### Step 2: Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e6fb74e-c02b-40cf-add4-798a3210b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "X_standardized = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856a315-d502-4d39-9d47-372c6160e695",
   "metadata": {},
   "source": [
    "### Step 3: Compute the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b6c97de-e19d-4f5f-96bd-f47bb50ff152",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(X_standardized.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b0922-ae22-4de8-830b-653af7e148b8",
   "metadata": {},
   "source": [
    "### Step 4: Compute eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c10a29e-ae99-49e5-b56b-60675582f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3112af-ffba-43c7-8079-c447b4638033",
   "metadata": {},
   "source": [
    "### Step 5: Sort eigenvalues and eigenvectors in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6a34175-58bc-478f-bf46-2b9539eb7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3793cd-3711-43b1-82b7-89386958cac8",
   "metadata": {},
   "source": [
    "### Step 6: Project data onto the top 4 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "009ee691-0c17-49c4-98f9-3511539d7b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.2052705  0.19121186 0.17205714 0.10949081 0.09226028]\n"
     ]
    }
   ],
   "source": [
    "k = 5  \n",
    "top_eigenvectors = eigenvectors[:, :k]\n",
    "X_pca = np.dot(X_standardized, top_eigenvectors)\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fd9a495-1274-4179-b50b-bba92eb0a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2ca5-16d5-4795-b1d5-253335547d73",
   "metadata": {},
   "source": [
    "i did pca above \n",
    "# final Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03fcf706-e6a2-485b-b108-06457edcd87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.67%\n",
      "Precision: 70.61%\n",
      "Recall: 71.64%\n",
      "F1 Score: 71.12%\n"
     ]
    }
   ],
   "source": [
    "def hinge_loss(w, X, y, C=1):\n",
    "    return 0.5 * np.dot(w, w) + C * np.sum(np.maximum(0, 1 - y * (np.dot(X, w))))\n",
    "\n",
    "def gradient(w, X, y, C=1):\n",
    "    return w - C * np.dot(X.T, (y * (np.maximum(0, 1 - y * np.dot(X, w)))))\n",
    "\n",
    "def train_svm(X, y, learning_rate=0.001, epochs=1000, C=1):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    y = 2 * y - 1  # Convert target to {-1, 1}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        grad = gradient(w, X, y, C)\n",
    "        w -= learning_rate * grad\n",
    "        \n",
    "    return w\n",
    "w = train_svm(X, y, learning_rate=0.001, epochs=1000, C=1)\n",
    "\n",
    "y_pred = np.sign(np.dot(X, w))  # Predictions in -1, 1\n",
    "\n",
    "y_pred_binary = np.where(y_pred == -1, 0, 1)\n",
    "\n",
    "TP = np.sum((y_pred_binary == 1) & (y == 1))\n",
    "TN = np.sum((y_pred_binary == 0) & (y == 0))\n",
    "FP = np.sum((y_pred_binary == 1) & (y == 0))\n",
    "FN = np.sum((y_pred_binary == 0) & (y == 1))\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
