{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba5a81e-e582-4327-a2fb-9933ab88abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded and preprocessed.\n",
      "Creating and training model...\n",
      "Epoch 1, Batch 0/1562, Loss: 2.3097\n",
      "Epoch 1, Batch 50/1562, Loss: 0.7680\n",
      "Epoch 1, Batch 100/1562, Loss: 0.3055\n",
      "Epoch 1, Batch 150/1562, Loss: 0.2971\n",
      "Epoch 1, Batch 200/1562, Loss: 0.1328\n",
      "Epoch 1, Batch 250/1562, Loss: 0.0876\n",
      "Epoch 1, Batch 300/1562, Loss: 0.0852\n",
      "Epoch 1, Batch 350/1562, Loss: 0.1792\n",
      "Epoch 1, Batch 400/1562, Loss: 0.2912\n",
      "Epoch 1, Batch 450/1562, Loss: 0.0975\n",
      "Epoch 1, Batch 500/1562, Loss: 0.2299\n",
      "Epoch 1, Batch 550/1562, Loss: 0.0899\n",
      "Epoch 1, Batch 600/1562, Loss: 0.0430\n",
      "Epoch 1, Batch 650/1562, Loss: 0.1020\n",
      "Epoch 1, Batch 700/1562, Loss: 0.1524\n",
      "Epoch 1, Batch 750/1562, Loss: 0.0702\n",
      "Epoch 1, Batch 800/1562, Loss: 0.0075\n",
      "Epoch 1, Batch 850/1562, Loss: 0.0386\n",
      "Epoch 1, Batch 900/1562, Loss: 0.1104\n",
      "Epoch 1, Batch 950/1562, Loss: 0.1684\n",
      "Epoch 1, Batch 1000/1562, Loss: 0.0201\n",
      "Epoch 1, Batch 1050/1562, Loss: 0.0008\n",
      "Epoch 1, Batch 1100/1562, Loss: 0.1495\n",
      "Epoch 1, Batch 1150/1562, Loss: 0.1859\n",
      "Epoch 1, Batch 1200/1562, Loss: 0.0402\n",
      "Epoch 1, Batch 1250/1562, Loss: 0.0426\n",
      "Epoch 1, Batch 1300/1562, Loss: 0.0157\n",
      "Epoch 1, Batch 1350/1562, Loss: 0.0152\n",
      "Epoch 1, Batch 1400/1562, Loss: 0.0162\n",
      "Epoch 1, Batch 1450/1562, Loss: 0.0605\n",
      "Epoch 1, Batch 1500/1562, Loss: 0.0926\n",
      "Epoch 1, Batch 1550/1562, Loss: 0.0095\n",
      "\n",
      "Epoch 1/10\n",
      "Training Loss: 0.1668, Training Accuracy: 0.9466\n",
      "Validation Loss: 0.0671, Validation Accuracy: 0.9815\n",
      "\n",
      "Epoch 2, Batch 0/1562, Loss: 0.0955\n",
      "Epoch 2, Batch 50/1562, Loss: 0.0517\n",
      "Epoch 2, Batch 100/1562, Loss: 0.1128\n",
      "Epoch 2, Batch 150/1562, Loss: 0.0091\n",
      "Epoch 2, Batch 200/1562, Loss: 0.0163\n",
      "Epoch 2, Batch 250/1562, Loss: 0.1811\n",
      "Epoch 2, Batch 300/1562, Loss: 0.0154\n",
      "Epoch 2, Batch 350/1562, Loss: 0.0027\n",
      "Epoch 2, Batch 400/1562, Loss: 0.1303\n",
      "Epoch 2, Batch 450/1562, Loss: 0.0257\n",
      "Epoch 2, Batch 500/1562, Loss: 0.0799\n",
      "Epoch 2, Batch 550/1562, Loss: 0.0055\n",
      "Epoch 2, Batch 600/1562, Loss: 0.0983\n",
      "Epoch 2, Batch 650/1562, Loss: 0.1196\n",
      "Epoch 2, Batch 700/1562, Loss: 0.0661\n",
      "Epoch 2, Batch 750/1562, Loss: 0.1812\n",
      "Epoch 2, Batch 800/1562, Loss: 0.0093\n",
      "Epoch 2, Batch 850/1562, Loss: 0.0257\n",
      "Epoch 2, Batch 900/1562, Loss: 0.0036\n",
      "Epoch 2, Batch 950/1562, Loss: 0.0778\n",
      "Epoch 2, Batch 1000/1562, Loss: 0.0014\n",
      "Epoch 2, Batch 1050/1562, Loss: 0.2543\n",
      "Epoch 2, Batch 1100/1562, Loss: 0.1000\n",
      "Epoch 2, Batch 1150/1562, Loss: 0.0068\n",
      "Epoch 2, Batch 1200/1562, Loss: 0.2198\n",
      "Epoch 2, Batch 1250/1562, Loss: 0.0704\n",
      "Epoch 2, Batch 1300/1562, Loss: 0.0100\n",
      "Epoch 2, Batch 1350/1562, Loss: 0.0603\n",
      "Epoch 2, Batch 1400/1562, Loss: 0.0015\n",
      "Epoch 2, Batch 1450/1562, Loss: 0.0900\n",
      "Epoch 2, Batch 1500/1562, Loss: 0.0578\n",
      "Epoch 2, Batch 1550/1562, Loss: 0.0017\n",
      "\n",
      "Epoch 2/10\n",
      "Training Loss: 0.0531, Training Accuracy: 0.9835\n",
      "Validation Loss: 0.0591, Validation Accuracy: 0.9832\n",
      "\n",
      "Epoch 3, Batch 0/1562, Loss: 0.0150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 321\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 299\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating and training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m--> 299\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m train_cnn(\n\u001b[1;32m    300\u001b[0m     model, \n\u001b[1;32m    301\u001b[0m     X_train[:\u001b[38;5;241m50000\u001b[39m], \n\u001b[1;32m    302\u001b[0m     y_train[:\u001b[38;5;241m50000\u001b[39m], \n\u001b[1;32m    303\u001b[0m     X_train[\u001b[38;5;241m50000\u001b[39m:], \n\u001b[1;32m    304\u001b[0m     y_train[\u001b[38;5;241m50000\u001b[39m:],\n\u001b[1;32m    305\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    306\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    307\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m    311\u001b[0m plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies)\n",
      "Cell \u001b[0;32mIn[4], line 220\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    219\u001b[0m d_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;241m-\u001b[39m batch_y\n\u001b[0;32m--> 220\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(d_output, learning_rate)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 175\u001b[0m, in \u001b[0;36mCNN.backward\u001b[0;34m(self, d_output, learning_rate)\u001b[0m\n\u001b[1;32m    173\u001b[0m d_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2\u001b[38;5;241m.\u001b[39mbackward(d_layer)\n\u001b[1;32m    174\u001b[0m d_layer \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m relu_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_outputs[\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m--> 175\u001b[0m d_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mbackward(d_layer, learning_rate)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Backward pass through first conv block\u001b[39;00m\n\u001b[1;32m    178\u001b[0m d_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1\u001b[38;5;241m.\u001b[39mbackward(d_layer)\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mConvLayer.backward\u001b[0;34m(self, d_output, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters):\n\u001b[1;32m     36\u001b[0m             d_filters[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(input_slice \u001b[38;5;241m*\u001b[39m d_output[:, k, i, j][:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m             d_input[:, :, i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size, j:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_size] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters[k] \u001b[38;5;241m*\u001b[39m d_output[:, k, i, j][:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_filters\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d_input\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size, input_channels=1):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.input_channels = input_channels\n",
    "        self.filters = np.random.randn(num_filters, input_channels, filter_size, filter_size) / np.sqrt(filter_size * filter_size * input_channels)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.batch_size, self.input_channels, self.height, self.width = input.shape\n",
    "        output_height = self.height - self.filter_size + 1\n",
    "        output_width = self.width - self.filter_size + 1\n",
    "        \n",
    "        output = np.zeros((self.batch_size, self.num_filters, output_height, output_width))\n",
    "        \n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                input_slice = input[:, :, i:i+self.filter_size, j:j+self.filter_size]\n",
    "                for k in range(self.num_filters):\n",
    "                    output[:, k, i, j] = np.sum(input_slice * self.filters[k], axis=(1,2,3))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        d_filters = np.zeros_like(self.filters)\n",
    "        \n",
    "        for i in range(d_output.shape[2]):\n",
    "            for j in range(d_output.shape[3]):\n",
    "                input_slice = self.input[:, :, i:i+self.filter_size, j:j+self.filter_size]\n",
    "                for k in range(self.num_filters):\n",
    "                    d_filters[k] += np.sum(input_slice * d_output[:, k, i, j][:, None, None, None], axis=0)\n",
    "                    d_input[:, :, i:i+self.filter_size, j:j+self.filter_size] += \\\n",
    "                        self.filters[k] * d_output[:, k, i, j][:, None, None, None]\n",
    "        \n",
    "        self.filters -= learning_rate * d_filters\n",
    "        return d_input\n",
    "\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.batch_size, self.channels, self.height, self.width = input.shape\n",
    "        self.output_height = self.height // self.pool_size\n",
    "        self.output_width = self.width // self.pool_size\n",
    "        \n",
    "        output = np.zeros((self.batch_size, self.channels, self.output_height, self.output_width))\n",
    "        \n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                h_start = i * self.pool_size\n",
    "                h_end = h_start + self.pool_size\n",
    "                w_start = j * self.pool_size\n",
    "                w_end = w_start + self.pool_size\n",
    "                input_slice = input[:, :, h_start:h_end, w_start:w_end]\n",
    "                output[:, :, i, j] = np.max(input_slice, axis=(2, 3))\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_output):\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        \n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                h_start = i * self.pool_size\n",
    "                h_end = h_start + self.pool_size\n",
    "                w_start = j * self.pool_size\n",
    "                w_end = w_start + self.pool_size\n",
    "                \n",
    "                input_slice = self.input[:, :, h_start:h_end, w_start:w_end]\n",
    "                mask = input_slice == np.max(input_slice, axis=(2, 3))[:, :, None, None]\n",
    "                d_input[:, :, h_start:h_end, w_start:w_end] = mask * d_output[:, :, i, j][:, :, None, None]\n",
    "                \n",
    "        return d_input\n",
    "\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size)\n",
    "        self.bias = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_input = np.dot(d_output, self.weights.T)\n",
    "        d_weights = np.dot(self.input.T, d_output)\n",
    "        d_bias = np.sum(d_output, axis=0)\n",
    "        \n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "        return d_input\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    return -np.mean(np.sum(targets * np.log(predictions), axis=1))\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = ConvLayer(num_filters=16, filter_size=3, input_channels=1)\n",
    "        self.pool1 = MaxPoolLayer(pool_size=2)\n",
    "        self.conv2 = ConvLayer(num_filters=32, filter_size=3, input_channels=16)\n",
    "        self.pool2 = MaxPoolLayer(pool_size=2)\n",
    "        self.fc1 = FCLayer(32 * 5 * 5, 128)\n",
    "        self.fc2 = FCLayer(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure input has the correct shape (batch_size, channels, height, width)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[:, np.newaxis, :, :]\n",
    "            \n",
    "        # Save intermediate values for backpropagation\n",
    "        self.layer_outputs = []\n",
    "        \n",
    "        # First convolution block\n",
    "        x = self.conv1.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = relu(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        \n",
    "        # Second convolution block\n",
    "        x = self.conv2.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = relu(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = relu(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        self.layer_outputs.append(x)\n",
    "        \n",
    "        # Output probability distribution\n",
    "        x = softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        # Backward pass through fully connected layers\n",
    "        d_layer = d_output\n",
    "        d_layer = self.fc2.backward(d_layer, learning_rate)\n",
    "        d_layer *= relu_derivative(self.layer_outputs[-2])\n",
    "        d_layer = self.fc1.backward(d_layer, learning_rate)\n",
    "        \n",
    "        # Reshape back to conv dimensions\n",
    "        d_layer = d_layer.reshape(self.layer_outputs[5].shape)\n",
    "        \n",
    "        # Backward pass through second conv block\n",
    "        d_layer = self.pool2.backward(d_layer)\n",
    "        d_layer *= relu_derivative(self.layer_outputs[4])\n",
    "        d_layer = self.conv2.backward(d_layer, learning_rate)\n",
    "        \n",
    "        # Backward pass through first conv block\n",
    "        d_layer = self.pool1.backward(d_layer)\n",
    "        d_layer *= relu_derivative(self.layer_outputs[1])\n",
    "        d_layer = self.conv1.backward(d_layer, learning_rate)\n",
    "\n",
    "def train_cnn(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            # Get batch\n",
    "            batch_X = X_train[start_idx:end_idx]\n",
    "            batch_y = y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = cross_entropy_loss(predictions, batch_y)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_classes = np.argmax(predictions, axis=1)\n",
    "            true_classes = np.argmax(batch_y, axis=1)\n",
    "            correct += np.sum(predicted_classes == true_classes)\n",
    "            \n",
    "            # Backward pass\n",
    "            d_output = predictions - batch_y\n",
    "            model.backward(d_output, learning_rate)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i}/{n_batches}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        epoch_loss /= n_batches\n",
    "        epoch_accuracy = correct / (n_batches * batch_size)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        val_predictions = model.forward(X_val)\n",
    "        val_loss = cross_entropy_loss(val_predictions, y_val)\n",
    "        val_predicted_classes = np.argmax(val_predictions, axis=1)\n",
    "        val_true_classes = np.argmax(y_val, axis=1)\n",
    "        val_accuracy = np.mean(val_predicted_classes == val_true_classes)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Loss over epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "    ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load MNIST dataset\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "    X = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape images to (samples, height, width)\n",
    "    X = X.reshape(-1, 28, 28)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = 60000\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_onehot = np.zeros((len(y_train), 10))\n",
    "    y_test_onehot = np.zeros((len(y_test), 10))\n",
    "    y_train_onehot[np.arange(len(y_train)), y_train.astype(int)] = 1\n",
    "    y_test_onehot[np.arange(len(y_test)), y_test.astype(int)] = 1\n",
    "    \n",
    "    return X_train, y_train_onehot, X_test, y_test_onehot\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_test, y_test = load_and_preprocess_data()\n",
    "    print(\"Data loaded and preprocessed.\")\n",
    "    \n",
    "    # Create and train model\n",
    "    print(\"Creating and training model...\")\n",
    "    model = CNN()\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = train_cnn(\n",
    "        model, \n",
    "        X_train[:50000], \n",
    "        y_train[:50000], \n",
    "        X_train[50000:], \n",
    "        y_train[50000:],\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_predictions = model.forward(X_test)\n",
    "    test_loss = cross_entropy_loss(test_predictions, y_test)\n",
    "    test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a62d10-abd4-4f9c-aa0c-29d7dbc61ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
